hist(hour(result$replied_at), xlim = c(0,24), main = "LAPD - When people replies tweets", xlab = "Time in Hour UTC")
View(result)
LAPD_graph <- reply_network(namefilter = "LAPDHQ", by_type = "post_id")
LAPD_graph
name = "LAPDHQ"
result <- tweetReplyAnaz(filename, name)
View(result)
filename = "/Users//akira_6686/Dropbox/SUMMER_2015/TwitterAnaz/dataset/streaming2.csv"
result <- tweetReplyAnaz(filename, name)
View(result)
stream_reply <- stream[stream$in_reply_to_screen_name %in% "LAPDHQ", ]
View(stream_reply)
tweets <- stream[stream$user.screen_name %in% "LAPDHQ",]
View(tweets)
tweets_sub <- subset(tweets, select = c(object_id, created_at))
View(tweets_sub)
reply_count <- data.frame(table(replies_sub$in_reply_to_status_id))
View(reply_count)
tweets_sub <- merge(tweets_sub, reply_count, by.x = "object_id", by.y = "Var1", all.x = T)
View(tweets_sub)
tweets_sub <- merge(tweets_sub, reply_count, by.x = "object_id", by.y = "Var1", all = T)
View(tweets_sub)
reply_count <- data.frame(table(replies_sub$in_reply_to_status_id))
tweets_sub <- merge(tweets_sub, reply_count, by.x = "object_id", by.y = "Var1", all = T)
View(tweets_sub)
tweets_sub <- subset(tweets, select = c(object_id, created_at))
reply_count <- data.frame(table(replies_sub$in_reply_to_status_id))
tweets_sub <- merge(tweets_sub, reply_count, by.x = "object_id", by.y = "Var1", all = T)
View(tweets_sub)
View(tweets_sub)
tweets_replies <- merge(tweets_sub, replies_sub, by.x = "object_id", by.y = "in_reply_to_status_id", all = T)
View(tweets_replies)
tweets_sub$Freq[is.na(tweets_sub$Freq)] <- 0
tweets_replies <- merge(tweets_sub, replies_sub, by.x = "object_id", by.y = "in_reply_to_status_id", all = T)
tweets_replies$time_response <- difftime(tweets_replies$created_at.y, tweets_replies$created_at.x, units = "hour")
names(tweets_replies) <- c("object_id", "created_at", "reply_count", "replied_at", "hour_response_interval")
View(tweets_replies)
# Create generic function to analyze tweet replies
tweetReplyAnaz <- function (filename, namefilter) {
# Read stream data
stream <- read.csv(filename, header = T, sep = ";", stringsAsFactors = F)
stream$created_at <- strptime(stream$created_at, format = "%a %b %d %T %z %Y", tz = "UTC")
# Filter by name
# Get Replies
replies <- stream[stream$in_reply_to_screen_name %in% namefilter,]
# Get all tweets posted by  agencies
tweets <- stream[stream$user.screen_name %in% namefilter,]
# Subset only created_at columns
replies_sub <- subset(replies, select =  c(in_reply_to_status_id, created_at))
tweets_sub <- subset(tweets, select = c(object_id, created_at))
# Calculate No of replies
reply_count <- data.frame(table(replies_sub$in_reply_to_status_id))
tweets_sub <- merge(tweets_sub, reply_count, by.x = "object_id", by.y = "Var1", all = T)
# !!! Get only replies to tweets posted in that time, not count replies to another tweets
tweets_sub$Freq[is.na(tweets_sub$Freq)] <- 0
# Calculate when people reply
tweets_replies <- merge(tweets_sub, replies_sub, by.x = "object_id", by.y = "in_reply_to_status_id", all = T)
tweets_replies$time_response <- difftime(tweets_replies$created_at.y, tweets_replies$created_at.x, units = "hour")
names(tweets_replies) <- c("object_id", "created_at", "reply_count", "replied_at", "hour_response_interval")
# Return
return (tweets_replies)
}
par(mfcol = c(1,1))
name = "BaltimorePolice"
result <- tweetReplyAnaz(filename, name)
hist(hour(result$replied_at), xlim = c(0,24), main = "LAPD - When people replies tweets", xlab = "Time in Hour UTC")
ggplot(result, aes(x= hour(replied_at))) + geom_freqpoly() + scale_x_continuous(breaks = seq(0,24,1)) +
ggtitle("NYPD - When people reply tweets")
tweets_replies <- merge(tweets_sub, replies_sub, by.x = "object_id", by.y = "in_reply_to_status_id", all.x = T)
tweets_replies$time_response <- difftime(tweets_replies$created_at.y, tweets_replies$created_at.x, units = "hour")
names(tweets_replies) <- c("object_id", "created_at", "reply_count", "replied_at", "hour_response_interval")
View(tweets_replies)
View(replies_sub)
tweets_replies <- merge(tweets_sub, replies_sub, by.x = "object_id", by.y = "in_reply_to_status_id", all = T)
tweets_replies$time_response <- difftime(tweets_replies$created_at.y, tweets_replies$created_at.x, units = "hour")
names(tweets_replies) <- c("object_id", "created_at", "reply_count", "replied_at", "hour_response_interval")
View(tweets_replies)
tweets_replies <- merge(tweets_sub, replies_sub, by.x = "object_id", by.y = "in_reply_to_status_id", all = T)
View(tweets_replies)
tweets_replies$created_at.y == 2015-07-12 07:23:02
"
tweets_replies$created_at.y == 2015-07-12 07:23:02
""
""
tweets_replies$created_at.y == 2015-07-12 07:23:02
tweets_replies <- merge(tweets_sub, replies_sub, by.x = "object_id", by.y = "in_reply_to_status_id", all.x = T)
names = c("NYPDnews", "BaltimorePolice", "CLEpolice", "LAPDHQ")
result <- tweetReplyAnaz(filename, names)
View(result)
# Create generic function to analyze tweet replies
tweetReplyAnaz <- function (filename, namefilter) {
# Read stream data
stream <- read.csv(filename, header = T, sep = ";", stringsAsFactors = F)
stream$created_at <- strptime(stream$created_at, format = "%a %b %d %T %z %Y", tz = "UTC")
# Filter by name
# Get Replies
replies <- stream[stream$in_reply_to_screen_name %in% namefilter,]
# Get all tweets posted by  agencies
tweets <- stream[stream$user.screen_name %in% namefilter,]
# Subset only created_at columns
replies_sub <- subset(replies, select =  c(in_reply_to_status_id, created_at))
tweets_sub <- subset(tweets, select = c(object_id, created_at))
# Calculate No of replies
reply_count <- data.frame(table(replies_sub$in_reply_to_status_id))
tweets_sub <- merge(tweets_sub, reply_count, by.x = "object_id", by.y = "Var1", all = T)
# !!! Get only replies to tweets posted in that time, not count replies to another tweets
tweets_sub$Freq[is.na(tweets_sub$Freq)] <- 0
# Calculate when people reply
tweets_replies <- merge(tweets_sub, replies_sub, by.x = "object_id", by.y = "in_reply_to_status_id", all.x = T)
tweets_replies$time_response <- difftime(tweets_replies$created_at.y, tweets_replies$created_at.x, units = "hour")
names(tweets_replies) <- c("object_id", "created_at", "reply_count", "replied_at", "hour_response_interval")
# Return
return (tweets_replies)
}
name = "NYPDnews"
result <- tweetReplyAnaz(filename, name)
hist(hour(result$replied_at), xlim = c(0,24), main = "NYPD - When people replies tweets", xlab = "Time in Hour UTC")
hist(hour(result$replied_at), breaks = 30, xlim = c(0,24), main = "NYPD - When people replies tweets", xlab = "Time in Hour UTC")
par(mfcol = c(2,2))
hist(hour(result$replied_at), breaks = 30, xlim = c(0,24), main = "NYPD - When people replies tweets", xlab = "Time in Hour UTC")
name = "BaltimorePolice"
result <- tweetReplyAnaz(filename, name)
hist(hour(result$replied_at), breaks = 30, xlim = c(0,24), main = "BPD - When people replies tweets", xlab = "Time in Hour UTC")
name = "CLEpolice"
result <- tweetReplyAnaz(filename, name)
hist(hour(result$replied_at), breaks = 30, xlim = c(0,24), main = "CPD - When people replies tweets", xlab = "Time in Hour UTC")
name = "LAPDHQ"
result <- tweetReplyAnaz(filename, name)
hist(hour(result$replied_at), breaks = 30, xlim = c(0,24), main = "LAPD - When people replies tweets", xlab = "Time in Hour UTC")
name = "NYPDnews"
barplot(table(result$reply_count), xlab = "Number of Replies", ylab = "Number of Tweets",main = "NYPD - Replies Distribution")
View(result)
name = "NYPDnews"
result <- tweetReplyAnaz(filename, name)
View(result)
par(mfcol = c(2,2))
hist(hour(result$replied_at), breaks = 30, xlim = c(0,24), main = "LAPD - When people replies tweets", xlab = "Time in Hour UTC")
par(mfcol = c(2,2))
barplot(table(result$reply_count), xlab = "Number of Replies", ylab = "Number of Tweets",main = "NYPD - Replies Distribution")
name = "BaltimorePolice"
result <- tweetReplyAnaz(filename, name)
barplot(table(result$reply_count), xlab = "Number of Replies", ylab = "Number of Tweets",main = "BPD - Replies Distribution")
name = "CLEpolice"
result <- tweetReplyAnaz(filename, name)
barplot(table(result$reply_count), xlab = "Number of Replies", ylab = "Number of Tweets",main = "CPD - Replies Distribution")
name = "LAPDHQ"
result <- tweetReplyAnaz(filename, name)
barplot(table(result$reply_count), xlab = "Number of Replies", ylab = "Number of Tweets",main = "LAPD - Replies Distribution")
name = "NYPDnews"
result <- tweetReplyAnaz(filename, name)
hist(as.numeric(result$hour_response_interval), breaks = 100, xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies", main = "How soon Tweets get replies")
par(mfcol = c(2,2))
hist(as.numeric(result$hour_response_interval), breaks = 100, xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "NYPD - How soon Tweets get replies")
name = "BaltimorePolice"
result <- tweetReplyAnaz(filename, name)
hist(as.numeric(result$hour_response_interval), breaks = 100, xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "BPD - How soon Tweets get replies")
name = "CLEpolice"
result <- tweetReplyAnaz(filename, name)
hist(as.numeric(result$hour_response_interval), breaks = 100, xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "CPD - How soon Tweets get replies")
hist(as.numeric(result$hour_response_interval), breaks = 10, xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "CPD - How soon Tweets get replies")
hist(as.numeric(result$hour_response_interval), xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "CPD - How soon Tweets get replies")
par(mfcol = c(2,2))
name = "NYPDnews"
result <- tweetReplyAnaz(filename, name)
hist(as.numeric(result$hour_response_interval), xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "NYPD - How soon Tweets get replies")
name = "BaltimorePolice"
result <- tweetReplyAnaz(filename, name)
hist(as.numeric(result$hour_response_interval), xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "CPD - How soon Tweets get replies")
name = "CLEpolice"
result <- tweetReplyAnaz(filename, name)
par(mfcol = c(2,2))
name = "NYPDnews"
result <- tweetReplyAnaz(filename, name)
hist(as.numeric(result$hour_response_interval), xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "NYPD - How soon Tweets get replies")
name = "BaltimorePolice"
result <- tweetReplyAnaz(filename, name)
hist(as.numeric(result$hour_response_interval), xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "BPD - How soon Tweets get replies")
name = "CLEpolice"
result <- tweetReplyAnaz(filename, name)
hist(as.numeric(result$hour_response_interval), xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "CPD - How soon Tweets get replies")
name = "LAPDHQ"
result <- tweetReplyAnaz(filename, name)
hist(as.numeric(result$hour_response_interval), xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "LAPD - How soon Tweets get replies")
View(result)
hist(as.numeric(result$hour_response_interval), xlab = "Time Reponse", ylab = "Number of Replies",
main = "LAPD - How soon Tweets get replies")
as.numeric(result$hour_response_interval)
View(result)
name = "BaltimorePolice"
name = "NYPDnews"
name = "CLEpolice"
name = "LAPDHQ"
name = "NYPDnews"
result <- tweetReplyAnaz(filename, name)
hist(as.numeric(result$hour_response_interval), xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "NYPD - How soon Tweets get replies")
par(mfcol = c(2,2))
hist(as.numeric(result$hour_response_interval), xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "NYPD - How soon Tweets get replies")
name = "BaltimorePolice"
result <- tweetReplyAnaz(filename, name)
hist(as.numeric(result$hour_response_interval), xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "BPD - How soon Tweets get replies")
name = "CLEpolice"
result <- tweetReplyAnaz(filename, name)
hist(as.numeric(result$hour_response_interval), xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "CPD - How soon Tweets get replies")
name = "LAPDHQ"
#names = c("NYPDnews", "BaltimorePolice", "CLEpolice", "LAPDHQ")
result <- tweetReplyAnaz(filename, name)
hist(as.numeric(result$hour_response_interval), xlim = c(0,24), xlab = "Time Reponse", ylab = "Number of Replies",
main = "LAPD - How soon Tweets get replies")
plot(df, main = "Cumulative Frequency Replies Graph" )
ggplot(df, aes(time, ecdf)) + geom_line() + ylab("Number of Replies") + ggtitle("Cumulative Frequency Replies Graph") + xlab("Hour Interval")
name = "NYPDnews"
result <- tweetReplyAnaz(filename, name)
time_reponse = na.omit((as.numeric(result$time_response)))
df <- data.frame(time = time_reponse, ecdf = ecdf(time_reponse)(time_reponse) * length(time_reponse))
plot(df, xlab = "Hour Interval", ylab = "Number of Replies",
main = "NYPD - Cumulative Frequency Replies Graph" )
time_reponse = na.omit((as.numeric(result$time_response)))
time_reponse
result <- tweetReplyAnaz(filename, name)
View(result)
time_reponse = na.omit((as.numeric(result$time_response)))
time_reponse
df <- data.frame(time = time_reponse, ecdf = ecdf(time_reponse)(time_reponse) * length(time_reponse))
View(df)
class(time_reponse)
time_reponse
as.numeric(result$time_response)
time_reponse = na.omit((as.numeric(result$hour_response_interval)))
time_reponse
par(mfcol = c(2,2))
name = "NYPDnews"
result <- tweetReplyAnaz(filename, name)
time_reponse = na.omit((as.numeric(result$hour_response_interval)))
time_reponse
df <- data.frame(time = time_reponse, ecdf = ecdf(time_reponse)(time_reponse) * length(time_reponse))
plot(df, xlab = "Hour Interval", ylab = "Number of Replies",
main = "NYPD - Cumulative Frequency Replies Graph" )
name = "BaltimorePolice"
result <- tweetReplyAnaz(filename, name)
time_reponse = na.omit((as.numeric(result$hour_response_interval)))
df <- data.frame(time = time_reponse, ecdf = ecdf(time_reponse)(time_reponse) * length(time_reponse))
plot(df, xlab = "Hour Interval", ylab = "Number of Replies",
main = "BPD - Cumulative Frequency Replies Graph" )
name = "CLEpolice"
result <- tweetReplyAnaz(filename, name)
time_reponse = na.omit((as.numeric(result$hour_response_interval)))
df <- data.frame(time = time_reponse, ecdf = ecdf(time_reponse)(time_reponse) * length(time_reponse))
plot(df, xlab = "Hour Interval", ylab = "Number of Replies",
main = "CPD - Cumulative Frequency Replies Graph" )
name = "LAPDHQ"
result <- tweetReplyAnaz(filename, name)
time_reponse = na.omit((as.numeric(result$hour_response_interval)))
df <- data.frame(time = time_reponse, ecdf = ecdf(time_reponse)(time_reponse) * length(time_reponse))
plot(df, xlab = "Hour Interval", ylab = "Number of Replies",
main = "LAPD - Cumulative Frequency Replies Graph" )
download.file(url = "https://resources.lendingclub.com/LoanStats3d.csv.zip", destfile = "datasets/LoanStats3d.csv", method = "curl")
download.file(url = "https://resources.lendingclub.com/LoanStats3d.csv.zip", destfile = "datasets/LoanStats3d.csv.zip", method = "curl")
getwd
getwd()
unzip(zipfile = "datasets/LoanStats3d.csv.zip", exdir = "datasets", files = "LoanStats3d.csv", overwrite = T, )
unzip(zipfile = "datasets/LoanStats3d.csv.zip", exdir = "datasets", files = "LoanStats3d.csv", overwrite = T, )
loan3d <- read.csv("datasets/LoanStats3d.csv")
loan3d <- read.table("datasets/LoanStats3d.csv", skip = 1, sep = ",", stringsAsFactors = F)
loan3d <- read.table("datasets/LoanStats3d.csv", skip = 1, nrows = 84281, sep = ",", stringsAsFactors = F)
loan3d <- read.table("datasets/LoanStats3d.csv", skip = 1, nrows = 84280, sep = ",", stringsAsFactors = F)
loan3d <- read.table("datasets/LoanStats3d.csv", skip = 1, nrows = 84279, sep = ",", stringsAsFactors = F)
?read.table
loan3d <- read.table("datasets/LoanStats3d.csv", skip = 1, nrows = 84277, sep = ",", stringsAsFactors = F, header  = T, quote = "")
loan3d <- read.table("datasets/LoanStats3d.csv", skip = 1, nrows = 84277, sep = ",", stringsAsFactors = F, header  = T)
loand3d[84277]
loan3d[84277,]
names(loan3d)
summary(loan3d)
str(loan3d)
summary(loan3d)
View(loan3d)
sum(loan3d$loan_amnt == loan3d$funded_amnt)
unique(loan3d$term)
test <- gsub("%", "",x = loan3d$int_rate)
test[1]
test[13]
loan3d <- read.table("datasets/LoanStats3d.csv", skip = 1, nrows = 84277, sep = ",", stringsAsFactors = F, header  = T, strip.white = T)
summary(loan3d)
test <- gsub("%", "",x = loan3d$int_rate)
test[3]
test[4]
test <- as.integer(gsub("%", "",x = loan3d$int_rate))
test[3]
test <- as.numeric(gsub("%", "",x = loan3d$int_rate))
test[3]
test[4]
loan3d$int_rate <- as.numeric(gsub("%", "",x = loan3d$int_rate))
summary(loan3d)
View(loan3d)
unique(loan3d$home_ownership)
unique(loan3d$verification_status)
unique(loan3d$issue_d)
unique(loan3d$loan_status)
unique(loan3d$pymnt_plan)
loan3d$installment[1][loan3d$pymnt_plan= "y"]
loan3d$installment[1][loan3d$pymnt_plan== "y"]
unique(loan3d$installment[loan3d$pymnt_plan== "y"])
(loan3d$installment[10][loan3d$pymnt_plan== "y"])
sum(loan3d$installment[loan3d$pymnt_plan== "y"])
loan3d[loan3d$installment== 1017.17,]
loan3d[loan3d$installment== 1017.07,]
summary(loan3d$dti)
quantile(loan3d$dti)
quantile(loan3d$dti, probs =  0.1)
quantile(loan3d$dti, probs =  0.05)
quantile(loan3d$dti, probs =  0.01)
quantile(loan3d$dti, probs =  0.001)
summary(loan3d$delinq_2yrs)
summary(loan3d$collection_recovery_fee)
summary(loan3d$collections_12_mths_ex_med)
loan3d$desc[1]
loan3d$desc[2]
loan3d$desc[10]
loan3d$desc
unique(loan3d$desc)
unique(loan3d$emp_title)
length(unique(loan3d$emp_title))
summary(loan3d$initial_list_status)
unique(loan3d$initial_list_status)
unique(loan3d$is_inc_v)
unique(loan3d$out_prncp)
unique(loan3d$policy_code)
summary(loan3d$recoveries)
summary(loan3d$revol_bal)
summary(loan3d$title)
library(stream)
install.packages("stream")
library(stream)
stream <- DSD_Gaussians(k = 3, d = 3, noise = .05, p = c(.5, .3, .1))
install.packages("Rcpp")
library(stream)
file <- "Dropbox/SUMMER_2015/MINDLab/dataset/Suspicion 3001 RMS.csv"
file_name <- "Dropbox/SUMMER_2015/MINDLab/dataset/Suspicion 3001 RMS.csv"
stream_file <- DSD_ReadCSV(file_name, class = 93, header = T)
stream_file
get_points(stream_file, n = 4)
stream_file <- DSD_ReadCSV(file_name, header = T)
stream_file
get_points(stream_file, n = 4)
stream_file <- DSD_ReadCSV(file_name, header = T, class = 93, k = 4)
stream_file
get_points(stream_file, n = 4)
?read.table
stream_file <- DSD_ReadCSV(file_name, header = T, class = 93, k = 4, quote = "")
stream_file
get_points(stream_file, n = 4)
stream_file <- DSD_ReadCSV(file_name, header = T, k = 4, quote = "")
stream_file
get_points(stream_file, n = 4)
stream_scaled <- DSD_ScaleStream(stream_file, center = TRUE, scale = TRUE)
get_points(stream_file, n = 5)
stream_scaled <- DSD_ScaleStream(stream_file, center = TRUE, scale = TRUE)
stream_file <- DSD_ReadCSV(file_name, header = T, k = 4, quote = "")
stream_scaled <- DSD_ScaleStream(stream_file, center = TRUE, scale = TRUE)
qnorm(.95, 1100, 100/(sqrt(75)))
qnorm(.95, 1100, 75/(sqrt(100)))
> pbinom(6, size = 8, prob = 0.5, lower.tail = FALSE)
pbinom(6, size = 8, prob = 0.5, lower.tail = FALSE)
pbinom(7, size = 8, prob = 0.5)
pbinom(7, size = 8, prob = 0.5, lower.tail = T)
pbinom(7, size = 8, prob = 0.5, lower.tail = F)
pbinom(3, size = 2, prob = 0.5, lower.tail = F)
pbinom(3, size = 5, prob = 0.5, lower.tail = F)
ppois(10, lambda = 5 * 3)
1 / sqrt(12 * 1000)
round(pnorm(.51, mean = 0.5, sd = sqrt(1 / 12 * 100), lower.tail = FALSE), 3)
round(pnorm(.51, mean = 0.5, sd = sqrt(1 / 12 / 100), lower.tail = FALSE), 3)
1/12/100
1/12
1/12*100
pnorm(14, mean = 15, sd = sqrt(1/12/100), lower.tail = F)
pnorm(14, mean = 15, sd = sqrt(1/ 12 / 100), lower.tail = F)
pnorm(14, mean = 15, sd = sqrt(1/100), lower.tail = F)
setwd("datasciencecoursera/RepData_PeerAssessment1/")
unzip("activity.zip",files = "activity.csv")
unzip("activity.zip",files = "activity.csv")
activity <- read.csv("activity.csv")
View(activity)
unique(activity$steps)
activity <- read.csv("activity.csv", stringsAsFactors = F)
library(dplyr)
activity %>% group_by(date) %>% sum(steps, na.rm = T)
activity %>% group_by(date) %>% summarise(total = sum(steps, na.rm = T))
activity %>% group_by(date) %>% summarise(total = sum(steps, na.rm = T))
activity %>% group_by(date) %>% summarise(total = sum(steps, na.rm = T)) %>%
summarise(mean_total = mean(total))
activity <- activity %>% group_by(date) %>% summarise(total = sum(steps, na.rm = T))
str(activity)
activity <- read.csv("activity.csv", stringsAsFactors = F)
activity <- activity %>% group_by(date) %>% mutate(total = sum(steps, na.rm = T))
head(activity)
activity <- read.csv("activity.csv", stringsAsFactors = F)
activity_per_day <- activity %>% group_by(date) %>% mutate(total = sum(steps, na.rm = T))
activity_per_day <- activity %>% group_by(date) %>% summarise(total = sum(steps, na.rm = T))
activity_per_day <- activity %>% group_by(date) %>% summarise(totalStep = sum(steps, na.rm = T))
View(activity_per_day)
hist(activity_per_day$totalStep)
summary(activity_per_day$totalStep)
summary(activity_per_day$totalStep)$mean
summary(activity_per_day$totalStep)[4]
unique(activity$interval)
activity %>% group_by(interval) %>% summarise(total = sum(step, na.rm=T))
activity %>% group_by(as.character(interval)) %>% summarise(total = sum(step, na.rm=T))
activity %>% group_by(interval) %>% summarise(total = sum(steps, na.rm=T))
activity %>% group_by(interval) %>% summarise(total = mean(sum(steps, na.rm=T))
)
activity %>% group_by(interval) %>% summarise(total = sum(steps, na.rm=T)) %>%
group_by(interval) %>% summarise(meanSteps = mean(total))
activity %>% group_by(interval) %>% summarise(total = sum(steps, na.rm=T))
activity_per_interval <- activity %>% group_by(interval) %>% summarise(total = sum(steps, na.rm=T))
activity_per_interval
View(activity_per_interval)
length(activity$date)
activity_per_interval <- activity %>% group_by(interval) %>% summarise(meanSteps = sum(steps, na.rm=T)/length(date))
```
activity_per_interval <- activity %>% group_by(interval) %>% summarise(meanSteps = sum(steps, na.rm=T)/length(date))
```
activity %>% group_by(interval) %>% summarise(meanSteps = sum(steps, na.rm=T)/length(date))
activity_per_interval <- activity %>% group_by(interval) %>% summarise(meanSteps = sum(steps, na.rm=T)/length(date))
with(activity_per_interval, plot(interval, meanSteps, type = "l"))
sum(activity$step == NA)
sum(activity$steps == NA)
(activity$steps == NA)
sum(is.na(activity$steps ))
activity_per_interval
activity_per_interval$meanSteps[activity_per_interval$interval==activity$steps]
activity$steps[is.na(activity$steps)]
activity$steps[is.na(activity$steps)]
intervalMissingV <- activity$interval[is.na(activity$steps)]
intervalMissingV
match(intervalMissingV, activity_per_interval$interval)
activity_per_interval$meanSteps[match(intervalMissingV, activity_per_interval$interval),]
activity_per_interval$meanSteps[match(intervalMissingV, activity_per_interval$interval)]
test<- activity_per_interval$meanSteps[match(intervalMissingV, activity_per_interval$interval)]
cbind(intervalMissingV, test)
head(cbind(intervalMissingV, test))
View(activity_per_interval)
head(intervalMissingV)
head(intervalMissingV, 50)
head(cbind(intervalMissingV, test), 50)
unique(activity$steps[is.na(activity$steps)])
activity$steps[is.na(activity$steps)]<- activity_per_interval$meanSteps[match(intervalMissingV, activity_per_interval$interval)]
```
activity <- read.csv("activity.csv", stringsAsFactors = F)
activityComp <- activity
intervalMissingV <- activityComp$interval[is.na(activityComp$steps)]
activityComp$steps[is.na(activity$steps)]<- activity_per_interval$meanSteps[match(intervalMissingV, activity_per_interval$interval)]
activityComp_per_day <- activityComp %>% group_by(date) %>% summarise(totalSteps = sum(steps, na.rm = T))
hist(activityComp_per_day$totalSteps)
summary(activityComp_per_day$totalStep)
summary(activity_per_day$totalStep)
summary(activityComp_per_day$totalStep)
View(activityComp)
View(activityComp_per_day)
View(activityComp)
with(activity_per_interval, plot(interval, meanSteps, type = "l"))
unique(activity$interval)
sum(is.na(activity$steps ))
intervalMissingV
View(activity_per_interval)
activityComp <- activity
intervalMissingV <- activityComp$interval[is.na(activityComp$steps)]
activityComp$steps[is.na(activity$steps)]
unique(activityComp$steps[is.na(activity$steps)])
length(activityComp$steps[is.na(activity$steps)])
length(intervalMissingV)
length(activity_per_interval$meanSteps[match(intervalMissingV, activity_per_interval$interval)])
summary(activityComp_per_day$totalStep)
summary(activity_per_day$totalStep)
weekdays
?weekdays
weekdays(.leap.seconds)
?strptime
activityComp$steps <- strptime(activityComp$date, format = "%F")
activityComp <- activity
intervalMissingV <- activityComp$interval[is.na(activityComp$steps)]
activityComp$steps[is.na(activity$steps)]<- activity_per_interval$meanSteps[match(intervalMissingV, activity_per_interval$interval)]
activityComp$date <- strptime(activityComp$date, format = "%F")
weekdays(activity$date[1])
weekdays(activityComp$date[1])
weekdays(activityComp$date[2])
?weekday
?weekdays.Date
unique(activityComp$date)
unique(weekdays(activityComp$date))
activityComp$weekday <- weekdays(activityComp$date)
View(activityComp)
unique(activityComp$weekday)
weekday_list <- c("Monday", "Tuesday","Wednesday", "Thursday", "Friday")
activityComp$isWeekday <- ifelse(activityComp$weekday %in% weekday_list, "weekday", "weekend")
activityComp$isWeekday <- as.factor(ifelse(activityComp$weekday %in% weekday_list, "weekday", "weekend"))
library(ggplot2)
